{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.0.cloudera2\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.4.3 (default, Nov 17 2016 01:08:31)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--jars /home/pklemenkov/hsu/lectures/kafka-clients-0.10.0.1.jar,/home/pklemenkov/hsu/lectures/spark-sql-kafka-0-10_2.11-2.3.0.jar pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='python3'\n",
    "os.environ[\"SPARK_HOME\"]='/opt/cloudera/parcels/SPARK2/lib/spark2/'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.6-src.zip'))\n",
    "os.environ[\"PYSPARK_PYTHON\"] = 'python3'\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = PipelineModel.load(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KAFKA_SERVERS = (\n",
    "    \"virtual-node01.bigdatateam.ru:9092\",\n",
    "    \"virtual-node02.bigdatateam.ru:9092\",\n",
    "    \"virtual-node03.bigdatateam.ru:9092\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = spark.readStream.format(\"kafka\")\\\n",
    "                       .option(\"kafka.bootstrap.servers\", \",\".join(KAFKA_SERVERS))\\\n",
    "                       .option(\"startingOffsets\", \"earliest\")\\\n",
    "                       .option(\"subscribe\", \"amazon_reviews3\")\\\n",
    "                       .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test.withColumn('parsed', test.value.cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('parsed').show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "data = df.select([get_json_object(df['parsed'],\"$.asin\").alias('asin'),\n",
    "                  get_json_object(df['parsed'],\"$.reviewerID\").alias('reviewerID'),\n",
    "                  get_json_object(df['parsed'],\"$.reviewText\").alias('reviewText')\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.select(col('json.reviewText').alias('reviewText'))\n",
    "data.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingRates = pipeline_model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingRates.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def create_submission(predictions):\n",
    "    selection = predictions.select([\"asin\", \"reviewerID\", \"prediction\"])\n",
    "    selection = selection.withColumn('id', sf.concat(sf.col('asin'),sf.lit('+'), sf.col('reviewerID')))\n",
    "    selection = selection.drop(\"asin\")\n",
    "    selection = selection.drop(\"reviewerID\")\n",
    "    \n",
    "    solution = selection.select(col(\"id\"), col(\"prediction\").alias(\"rating\"))\n",
    "    solution = solution.repartition(1)\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataStreaming = create_submission(streamingRates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query = dataStreaming.writeStream\\\n",
    "                                .format(\"csv\")\\\n",
    "                                .outputMode(\"append\")\\\n",
    "                                .option(\"checkpointLocation\", \"/user/hsp201807/checkpoint/\")\\\n",
    "                                .start('predictions_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming_query.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
