{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.0.cloudera2\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.4.3 (default, Nov 17 2016 01:08:31)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='python3'\n",
    "os.environ[\"SPARK_HOME\"]='/opt/cloudera/parcels/SPARK2/lib/spark2/'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.6-src.zip'))\n",
    "os.environ[\"PYSPARK_PYTHON\"] = 'python3'\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://virtual-client.bigdatateam.ru:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.cloudera2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f40775b1dd8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kmeans](pics/kmeans.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kmeans_algo](pics/kmeans_algo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/pklemenkov/hsu/lectures/lecture03/toxic_comment/train.csv\")\n",
    "df.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"comment_text\", StringType()),\n",
    "    StructField(\"toxic\", IntegerType()),\n",
    "    StructField(\"severe_toxic\", IntegerType()),\n",
    "    StructField(\"obscene\", IntegerType()),\n",
    "    StructField(\"threat\", IntegerType()),\n",
    "    StructField(\"insult\", IntegerType()),\n",
    "    StructField(\"identity_hate\", IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = spark.createDataFrame(df, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.repartition(4).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, comment_text: string, toxic: int, severe_toxic: int, obscene: int, threat: int, insult: int, identity_hate: int]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"comment_text\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = StopWordsRemover.loadDefaultStopWords(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words_filtered\", stopWords=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(inputCol=swr.getOutputCol(), outputCol=\"word_vector\", vocabSize=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessing = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    swr,\n",
    "    count_vectorizer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessing_model = preprocessing.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessed_dataset = preprocessing_model.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(word_vector=SparseVector(200, {0: 2.0, 1: 2.0, 5: 1.0, 53: 1.0, 55: 2.0, 68: 1.0, 101: 1.0, 115: 1.0, 129: 1.0})),\n",
       " Row(word_vector=SparseVector(200, {0: 1.0, 2: 2.0, 12: 1.0, 14: 1.0, 52: 1.0, 142: 1.0, 166: 1.0, 178: 1.0})),\n",
       " Row(word_vector=SparseVector(200, {})),\n",
       " Row(word_vector=SparseVector(200, {0: 2.0, 2: 1.0, 3: 1.0, 57: 1.0, 62: 1.0, 70: 1.0})),\n",
       " Row(word_vector=SparseVector(200, {12: 1.0, 24: 1.0, 153: 1.0}))]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_dataset.select([\"word_vector\"]).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, comment_text: string, toxic: int, severe_toxic: int, obscene: int, threat: int, insult: int, identity_hate: int]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(featuresCol=\"word_vector\", k=7, seed=5757)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans_model = kmeans.fit(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clustering = kmeans_model.transform(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=0),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=0),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=0),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=0),\n",
       " Row(toxic=1, severe_toxic=1, obscene=1, threat=0, insult=1, identity_hate=1, prediction=0),\n",
       " Row(toxic=1, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=0),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=0),\n",
       " Row(toxic=0, severe_toxic=0, obscene=1, threat=0, insult=1, identity_hate=0, prediction=0),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=0),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=0)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering[clustering.columns[2:8] + [\"prediction\"]].take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = ClusteringEvaluator(featuresCol=\"word_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5023566031104862"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(comment_text='\"Contents of the library (objects and functions to be used outside, situation\\nlate August 2004)\\n\\nClasses:\\nPage: A MediaWiki page\\n    __init__               Page(Site, Title) - the page with title Title on wikimedia site Site\\n    title                  The name of the page, in a form suitable for an interwiki link\\n    urlname                The name of the page, in a form suitable for a URL\\n    titleWithoutNamespace  The name of the page, with the namespace part removed\\n    section                The section of the page (the part of the name after \\'#\\')\\n    sectionFreeTitle       The name without the section part\\n    aslink                 The name of the page in the form Title or lang:Title\\n    site                   The wiki this page is in\\n    encoding               The encoding of the page\\n    isAutoTitle            If the title is a well known, auto-translatable title\\n    autoFormat             Returns (dictName, value), where value can be a year, date, etc.,\\n                            and dictName is \\'YearBC\\', \\'December\\', etc.\\n    isCategory             True if the page is a category, false otherwise\\n    isImage                True if the page is an image, false otherwise\\n\\n    get (*)                The text of the page\\n    exists (*)             True if the page actually exists, false otherwise\\n    isRedirectPage (*)     True if the page is a redirect, false otherwise\\n    isEmpty (*)            True if the page has 4 characters or less content, not\\n                            counting interwiki and category links\\n    botMayEdit (*)         True if bot is allowed to edit page\\n    interwiki (*)          The interwiki links from the page (list of Pages)\\n    categories (*)         The categories the page is in (list of Pages)\\n    linkedPages (*)        The normal pages linked from the page (list of Pages)\\n    imagelinks (*)         The pictures on the page (list of ImagePages)\\n    templates (*)          All templates referenced on the page (list of strings)\\n    getRedirectTarget (*)  The page the page redirects to\\n    isDisambig (*)         True if the page is a disambiguation page\\n    getReferences          List of pages linking to the page\\n    namespace              The namespace in which the page is\\n    permalink (*)          The url of the permalink of the current version\\n    move                   Move the page to another title\\n    put(newtext)           Saves the page\\n    put_async(newtext)     Queues the page to be saved asynchronously\\n    delete                 Deletes the page (requires being logged in)\\n\\n    (*)  This loads the page if it has not been loaded before; permalink might\\n          even reload it if it has been loaded before\\n\\nSite: a MediaWiki site\\n    messages               There are new messages on the site\\n    forceLogin()           Does not continue until the user has logged in to\\n                            the site\\n    getUrl()               Retrieve an URL from the site\\n    mediawiki_message(key): Retrieve the text of the MediaWiki message with\\n                            the key \"\"key\"\"\\n    has_mediawiki_message(key)  True if this site defines a MediaWiki message\\n                                 with the key \"\"key\"\"\\n    Special pages:\\n        Dynamic pages:\\n            allpages(): Special:Allpages\\n            newpages(): Special:Newpages\\n            longpages(): Special:Longpages\\n            shortpages(): Special:Shortpages\\n            categories(): Special:Categories\\n\\n        Cached pages:\\n            deadendpages(): Special:Deadendpages\\n            ancientpages(): Special:Ancientpages\\n            lonelypages(): Special:Lonelypages\\n            uncategorizedcategories(): Special:Uncategorizedcategories\\n            uncategorizedpages(): Special:Uncategorizedpages\\n            unusedcategories(): Special:Unusuedcategories\\n\\nOther functions:\\ngetall(): Load pages via Special:Export\\nsetAction(text): Use \\'text\\' instead of \"\"Wikipedia python library\"\" in\\n    editsummaries\\nhandleArgs(): Checks whether text is an argument defined on wikipedia.py\\n    (these are -family, -lang, -log and others)\\ntranslate(xx, dict): dict is a dictionary, giving text depending on language,\\n    xx is a language. Returns the text in the most applicable language for\\n    the xx: wiki\\nsetUserAgent(text): Sets the string being passed to the HTTP server as\\n    the User-agent: header. Defaults to \\'Pywikipediabot/1.0\\'.\\n\\noutput(text): Prints the text \\'text\\' in the encoding of the user\\'s console.NEWL'),\n",
       " Row(comment_text='Lots More Abstracts\\nSmithsonian/NASA Astrophysics Data System (ADS)\\nQuery Results from the Instrumentation Database\\nRetrieved 100 abstracts, starting with number 1. Total number selected: 637550.\\n-\\n Bibcode \\n Score Date List of Links \\n Authors \\n Title \\n Access Control Help \\n\\n \\n1  1977OSAJ...67..399G \\n 1.000 03/1977 A                                              C                      U       \\n Grosso, R. P.; Yellin, M. \\n The membrane mirror as an adaptive optical element \\n\\n \\n2  1976SPIE...75...97Y \\n 0.456 00/1976 A                                  T  M                                       \\n Yellin, M. \\n Using membrane mirrors in adaptive optics \\n\\n \\n3  1993SPIE.1945..421M \\n 0.367 11/1993 A                                  T  M                              U       \\n Miller, Linda M.; Agronin, Michael L.; Bartman, Randall K.; Kaiser, William J.; Kenny, Thomas W.; Norton, Robert L.; Vote, Erika C. \\n Fabrication and characterization of a micromachined deformable mirror for adaptive optics applications \\n\\n \\n4  1991SPIE.1542..165C \\n 0.331 12/1991 A                                  T  M      C                               \\n Clampin, M.; Durrance, S. T.; Golimowski, D. A.; Barkhouser, R. H. \\n The Johns Hopkins Adaptive Optics Coronagraph \\n\\n \\n5  2000SPIE.4091...83Y \\n 0.322 10/2000 A                                  T  M                              U       \\n Yang, Eui-Hyeok; Wiberg, Dean V.; Dekany, Richard G. \\n Design and fabrication of electrostatic actuators with corrugated membranes for MEMS deformable mirror in space \\n\\n \\n6  2000SPIE.4075...41R \\n 0.321 09/2000 A                                  T  M                              U       \\n Ross, Alan W.; Graham, Stephen C.; Gundlach, Alan M.; Stevenson, J. Tom M.; Hossack, William J.; Vass, David G.; Bodammer, Georg; Smith, Euan; Ward, Kevin \\n Microfabrication and packaging of deformable mirror devices \\n\\n \\n7  1994SPIE.2201..762T \\n 0.315 05/1994 A                                  T  M      C                      U       \\n Takami, Hideki; Iye, Masanori \\n Membrane deformable mirror for SUBARU adaptive optics \\n\\n \\n8  1999SPIE.3785..160W \\n 0.315 10/1999 A                                  T  M                                       \\n Winsor, Robert S.; Sivaramakrishnan, Anand; Makidon, Russell B. \\n Finite element analysis of low-cost membrane deformable mirrors for high-order adaptive optics \\n\\n \\n9  2000SPIE.4348..348B \\n 0.302 02/2000 A                                  T  M                                       \\n Borovkov, Alexei I.; Pyatishev, Evgenij N.; Lurie, Mihail S.; Korshunov, Andrey V.; Akulshin, Y. D.; Dolganov, A. G.; Sabadash, V. O. \\n Micronozzles: 3D numerical structural and gas dynamics modeling, fabrication, and preliminary experimental results \\n\\n \\n10  1999SPIE.3591..137Z \\n 0.298 06/1999 A                                  T  M                                       \\n Zhu, Lijun; Sun, Pang Chen; Bartsch, Dirk-Uwe G.; Freeman, William R.; Fainman, Yeshaiahu \\n Adaptive fundus imaging using a micromachined membrane deformable mirror \\n\\n \\n11  2001SPIE.4327...13W \\n 0.297 08/2001 A                                  T  M      C                               \\n Wagner, John W.; Agnes, Gregory S. \\n Optical metrology of adaptive membrane mirrors \\n\\n \\n12  2002OptEn..41..561P \\n 0.297 03/2002 A      E  F                              R                          U       \\n Perreault, Julie A.; Bifano, Thomas G.; Levine, Bruce M.; Horenstein, Mark N. \\n Adaptive optic'),\n",
       " Row(comment_text='I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg'),\n",
       " Row(comment_text='ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nAN'),\n",
       " Row(comment_text='....... \\nnow that i think about it, luna means moon and satin means devil!\\nso that means you are the moon devil who else would be the moon devil\\nbut the person who disabled my ACCOUNT!!! ( if trevor is reading this \\nthen i want to say that i love that snowman, i tred to make one but it\\ndidnt work(scroll down))\\n\\nfurther\\n\\neven further\\n\\nalmost there....\\n\\n __\\n(**)\\n( .)\\n(. )\\n                          O   \\n                         O O                  \\n                        O   O                        \\n                       O     O                            \\n                      O       O                      \\n                     O         O                       \\n                    O           O                     \\n                   O             O               \\n                  O               O                                                             \\n                 O                 O          \\n                O                   O       \\n               O                     O         \\n              O                       O         \\n             O                         O\\n            OOOOOOOOOOOOOOOOOOOOOOOOOOOO                               \\n           O  O                      O  O                             \\n          O    O                    O    O                    \\n         O      O                  O      O                        \\n        O        O                O        O                         \\n       O          O              O          O               \\n      O            O            O            O              \\n     O              O          O              O     \\n    O                O        O                O                     \\n   O                  O      O                  O                                    \\n  O                    O    O                    O                                               \\n O                      O  O                      O                          \\nO                        OO                        O \\nOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO  SKILLS! IT TOOK 20 MINUTES TOO MAKE')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering.filter(clustering.prediction == 1)[[\"comment_text\"]].take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(featuresCol=\"word_vector\", k=2, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans_model = kmeans.fit(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clustering = kmeans_model.transform(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9955064024878033"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.35142132e+02, 5.78680203e-01, 9.61928934e-01, 1.08375635e+00,\n",
       "        7.41116751e-01, 1.34263959e+00, 8.29949239e-01, 2.03045685e+00,\n",
       "        7.10659898e-01, 7.00507614e-01, 6.14213198e-01, 7.10659898e-01,\n",
       "        5.05076142e-01, 1.45177665e+00, 4.13705584e-01, 4.03553299e-01,\n",
       "        4.77157360e-01, 4.01015228e-01, 4.21319797e-01, 3.37563452e-01,\n",
       "        4.49238579e-01, 2.96954315e-01, 4.23857868e-01, 2.71573604e-01,\n",
       "        4.31472081e-01, 3.65482234e-01, 4.61928934e-01, 5.00000000e-01,\n",
       "        2.81725888e-01, 2.28426396e-01, 4.89847716e-01, 3.04568528e-01,\n",
       "        3.09644670e-01, 2.51269036e-01, 2.20812183e-01, 3.29949239e-01,\n",
       "        2.05583756e-01, 2.30964467e-01, 4.26395939e-01, 2.30964467e-01,\n",
       "        1.01522843e+00, 2.91878173e-01, 1.97969543e-01, 2.28426396e-01,\n",
       "        2.05583756e-01, 4.97461929e-01, 1.87817259e-01, 2.58883249e-01,\n",
       "        2.51269036e-01, 3.19796954e-01, 1.72588832e-01, 2.33502538e-01,\n",
       "        2.46192893e-01, 2.53807107e-01, 2.18274112e-01, 3.02030457e-01,\n",
       "        2.25888325e-01, 1.75126904e-01, 2.00507614e-01, 2.15736041e-01,\n",
       "        2.10659898e-01, 1.59898477e-01, 1.09137056e-01, 1.72588832e-01,\n",
       "        1.97969543e-01, 1.85279188e-01, 9.13705584e-02, 1.11675127e-01,\n",
       "        1.42131980e-01, 1.97969543e-01, 2.36040609e-01, 2.30964467e-01,\n",
       "        2.18274112e-01, 1.77664975e-01, 1.42131980e-01, 2.05583756e-01,\n",
       "        3.60406091e-01, 1.64974619e-01, 2.10659898e-01, 3.27411168e-01,\n",
       "        1.37055838e-01, 1.44670051e-01, 1.06598985e-01, 1.21827411e-01,\n",
       "        1.31979695e-01, 2.25888325e-01, 2.46192893e-01, 2.41116751e-01,\n",
       "        2.38578680e-01, 1.97969543e-01, 2.18274112e-01, 1.75126904e-01,\n",
       "        1.01522843e-01, 1.14213198e-01, 1.06598985e-01, 1.75126904e-01,\n",
       "        1.31979695e-01, 1.52284264e-01, 1.09137056e-01, 1.49746193e-01,\n",
       "        9.39086294e-02, 1.47208122e-01, 1.64974619e-01, 4.97461929e-01,\n",
       "        1.37055838e-01, 8.45177665e-01, 2.74111675e-01, 1.47208122e-01,\n",
       "        1.11675127e-01, 1.70050761e-01, 2.96954315e-01, 1.21827411e-01,\n",
       "        1.24365482e-01, 2.81725888e-01, 1.52284264e-01, 2.03045685e-01,\n",
       "        5.17766497e-01, 1.72588832e-01, 1.01522843e-01, 1.37055838e-01,\n",
       "        2.10659898e-01, 5.07614213e-02, 1.29441624e-01, 1.97969543e-01,\n",
       "        1.80203046e-01, 7.86802030e-02, 8.88324873e-02, 2.81725888e-01,\n",
       "        1.82741117e-01, 1.26903553e-01, 1.49746193e-01, 1.04060914e-01,\n",
       "        1.37055838e-01, 1.54822335e-01, 1.75126904e-01, 1.77664975e-01,\n",
       "        1.92893401e-01, 1.01522843e-01, 1.42131980e-01, 9.89847716e-02,\n",
       "        2.03045685e-01, 4.06091371e-02, 1.62436548e-01, 1.21827411e-01,\n",
       "        8.88324873e-02, 1.16751269e-01, 1.19289340e-01, 8.62944162e-02,\n",
       "        1.87817259e-01, 1.95431472e-01, 9.39086294e-02, 5.32994924e-02,\n",
       "        1.39593909e-01, 7.53807107e-01, 1.26903553e-01, 1.34517766e-01,\n",
       "        1.64974619e-01, 1.26903553e-01, 4.82233503e-02, 1.92893401e-01,\n",
       "        8.12182741e-02, 1.01522843e-01, 1.11675127e-01, 9.89847716e-02,\n",
       "        1.92893401e-01, 1.26903553e-01, 9.89847716e-02, 1.16751269e-01,\n",
       "        1.14213198e-01, 1.57360406e-01, 5.32994924e-02, 5.32994924e-02,\n",
       "        1.29441624e-01, 8.12182741e-02, 1.09137056e-01, 1.64974619e-01,\n",
       "        9.13705584e-02, 1.09137056e-01, 1.14213198e-01, 1.21827411e-01,\n",
       "        1.16751269e-01, 1.42131980e-01, 7.61421320e-02, 1.09137056e-01,\n",
       "        4.31472081e-02, 7.10659898e-02, 1.26903553e-01, 6.09137056e-02,\n",
       "        1.39593909e-01, 1.44670051e-01, 1.11675127e-01, 7.86802030e-02,\n",
       "        1.19289340e-01, 1.21827411e-01, 1.75126904e-01, 9.13705584e-02,\n",
       "        1.09137056e-01, 1.09137056e-01, 1.11675127e-01, 1.24365482e-01]),\n",
       " array([2.80022868, 0.51164427, 0.24272979, 0.1777895 , 0.1710046 ,\n",
       "        0.16213398, 0.15270422, 0.14179184, 0.13920353, 0.13893967,\n",
       "        0.11765519, 0.11616   , 0.10159131, 0.0992166 , 0.09486295,\n",
       "        0.0940651 , 0.09321698, 0.09006326, 0.08141252, 0.08068377,\n",
       "        0.07834675, 0.07082053, 0.06965202, 0.06831389, 0.06412987,\n",
       "        0.06238967, 0.06152899, 0.06115205, 0.05983905, 0.05887157,\n",
       "        0.05703713, 0.05704342, 0.05629582, 0.05492628, 0.05434202,\n",
       "        0.05397137, 0.05398393, 0.05299761, 0.05219347, 0.05237566,\n",
       "        0.05042186, 0.05216834, 0.05215578, 0.05153383, 0.05103752,\n",
       "        0.05006377, 0.04974337, 0.04878217, 0.04867537, 0.04804086,\n",
       "        0.04782726, 0.04723044, 0.04715505, 0.04703569, 0.04699171,\n",
       "        0.04546511, 0.04536459, 0.04501278, 0.04490599, 0.0447929 ,\n",
       "        0.04420865, 0.04393223, 0.04387569, 0.04362439, 0.04295219,\n",
       "        0.04293962, 0.04277   , 0.04210407, 0.04072825, 0.04052721,\n",
       "        0.04026964, 0.04011258, 0.03984244, 0.03972936, 0.03962884,\n",
       "        0.03835353, 0.03768761, 0.0381525 , 0.03754311, 0.03666359,\n",
       "        0.03705937, 0.03692116, 0.03686462, 0.03666359, 0.03653166,\n",
       "        0.03614844, 0.03597882, 0.0359851 , 0.03597882, 0.03599766,\n",
       "        0.03576522, 0.03583432, 0.03570239, 0.0353443 , 0.03531917,\n",
       "        0.03514327, 0.03482915, 0.03453388, 0.03387424, 0.03364808,\n",
       "        0.03313921, 0.03256124, 0.03189531, 0.03071424, 0.03151209,\n",
       "        0.02943893, 0.0306577 , 0.03093412, 0.03078962, 0.03062   ,\n",
       "        0.03014255, 0.03050692, 0.03035614, 0.02992266, 0.03007344,\n",
       "        0.02990382, 0.02903058, 0.02984728, 0.02996036, 0.02957714,\n",
       "        0.02920648, 0.02949547, 0.02925046, 0.02899916, 0.02891121,\n",
       "        0.02914994, 0.02900545, 0.02848401, 0.0285594 , 0.02867248,\n",
       "        0.02828298, 0.02826413, 0.02789348, 0.02781181, 0.02773014,\n",
       "        0.02766731, 0.02746628, 0.02769244, 0.02751654, 0.02758564,\n",
       "        0.02723383, 0.0274223 , 0.02695113, 0.02695741, 0.02680664,\n",
       "        0.02644226, 0.02634803, 0.02636059, 0.0260025 , 0.02578262,\n",
       "        0.02601506, 0.02607161, 0.02575749, 0.02414922, 0.02544966,\n",
       "        0.02527375, 0.02510413, 0.02517323, 0.02533658, 0.02484655,\n",
       "        0.02498477, 0.02475232, 0.02448218, 0.02440051, 0.02390421,\n",
       "        0.02384138, 0.02352099, 0.02342675, 0.02333252, 0.02320687,\n",
       "        0.02316289, 0.02315661, 0.02266659, 0.02276711, 0.0224844 ,\n",
       "        0.02234619, 0.02244671, 0.02227081, 0.02221426, 0.02218285,\n",
       "        0.0220949 , 0.02199438, 0.02212631, 0.0219881 , 0.02196925,\n",
       "        0.02188758, 0.02171796, 0.02174309, 0.02141641, 0.02134731,\n",
       "        0.02134731, 0.02137872, 0.02122166, 0.02115255, 0.02095152,\n",
       "        0.02112742, 0.02097665, 0.02091383, 0.02086985, 0.02080074])]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_model.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135.14213197969542"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(kmeans_model.clusterCenters()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   7,  13,   5,   3,  40,   2, 105,   6, 153,   4,  11,   8,\n",
       "         9,  10,   1, 116,  12,  27, 103,  45,  30,  16,  26,  20,  24,\n",
       "        38,  22,  18,  14,  15,  17,  25,  76,  19,  35,  79,  49,  32,\n",
       "        31,  55,  21, 110,  41, 113,  28, 127, 106,  23,  47,  53,  33,\n",
       "        48,  86,  52,  87,  88,  70,  51,  39,  37,  71,  29,  43,  85,\n",
       "        56,  34,  90,  54,  72,  59,  78, 120,  60,  75,  44,  36, 140,\n",
       "       115,  58, 123,  42,  64,  89,  69, 149, 136, 159, 164,  46, 148,\n",
       "        65, 128, 124,  73, 135, 194,  57, 134,  95,  91,  63,  50, 117,\n",
       "       109, 175, 156, 102,  77, 142,  61, 169, 133, 114,  97, 130,  99,\n",
       "       107, 101,  81, 189, 138,  74, 181,  68, 188, 152, 119, 104,  80,\n",
       "       132, 155,  96,  84, 122, 172, 129, 186, 154, 157, 165, 199, 112,\n",
       "       179, 193,  83, 111, 143, 192, 146, 145, 167, 180, 178, 168,  93,\n",
       "       198,  67, 190, 162, 108, 177,  98, 196, 197, 183,  62, 174,  82,\n",
       "        94, 131, 161, 137,  92, 118, 166, 163, 139, 100, 150, 176, 195,\n",
       "        66, 126, 144, 147, 173, 160, 191, 125, 182, 185, 187, 171, 170,\n",
       "       151, 121, 158, 184, 141])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(-kmeans_model.clusterCenters()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   3,   7, 194, 174,  36,   1,  56,  13, 117, 109,  91,  86,\n",
       "        55,  99,  12, 135, 166,  49,  20,  54,  18,  17,  10,  19,  63,\n",
       "        65,  71,   6,  75,  79,  80,  82,  85,   8, 101,  72,  15,   5,\n",
       "        11, 134, 142, 151, 152, 164, 165, 167,   4, 180, 181, 186,   2,\n",
       "       114, 129, 199,  26,  27,  39,  41,  42,  43,  30,  23,  22,  45,\n",
       "        50,  33, 140, 132, 160, 159, 158, 133, 157, 156, 155, 154, 153,\n",
       "        32,  29, 136, 137, 150, 149, 148, 161, 138, 147, 146, 145, 144,\n",
       "       143,  31, 141, 139, 162,  52,  28, 197, 196, 195,  16, 193, 192,\n",
       "       191, 190, 189, 188, 187,  21, 185, 184, 163, 183, 179, 178, 177,\n",
       "       176, 175,  24, 173, 172, 171, 170, 169, 168,  25, 131, 182, 130,\n",
       "       127, 128,  40,  90,  89,  88,  87,   9,  84,  83,  81,  44,  78,\n",
       "        77,  76,  74,  73,  46,  47,  51,  57,  58,  59,  60,  61,  92,\n",
       "        62,  48,  66,  67,  68,  69,  70,  64,  34,  93,  95,  53, 126,\n",
       "       125, 124, 123, 122, 121, 120, 119, 118,  35, 116, 115,  37, 113,\n",
       "       112, 111,  96,  97,  98, 198, 100, 102,  94, 103, 105, 106,  14,\n",
       "       108,  38, 110, 104, 107])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(-kmeans_model.clusterCenters()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '\"',\n",
       " 'article',\n",
       " 'page',\n",
       " 'please',\n",
       " 'like',\n",
       " 'one',\n",
       " '-',\n",
       " 'wikipedia',\n",
       " 'talk',\n",
       " 'think',\n",
       " 'see',\n",
       " 'also',\n",
       " 'know',\n",
       " 'may',\n",
       " 'edit',\n",
       " 'people',\n",
       " 'use',\n",
       " 'get',\n",
       " 'even',\n",
       " 'make',\n",
       " 'articles',\n",
       " 'good',\n",
       " 'want',\n",
       " 'time',\n",
       " 'it.',\n",
       " 'need',\n",
       " 'new',\n",
       " 'thank',\n",
       " 'go',\n",
       " 'first',\n",
       " 'information',\n",
       " 'many',\n",
       " 'made',\n",
       " 'find',\n",
       " 'page.',\n",
       " 'name',\n",
       " 'really',\n",
       " 'thanks',\n",
       " 'say',\n",
       " 'fuck',\n",
       " 'much',\n",
       " 'used',\n",
       " 'since',\n",
       " 'article.',\n",
       " 'user',\n",
       " 'add',\n",
       " 'way',\n",
       " 'take',\n",
       " 'help',\n",
       " 'sources',\n",
       " 'look',\n",
       " 'someone',\n",
       " 'still',\n",
       " 'read',\n",
       " 'section',\n",
       " 'pages',\n",
       " 'going',\n",
       " 'two',\n",
       " 'deletion',\n",
       " 'you.',\n",
       " 'source',\n",
       " 'edits',\n",
       " 'without',\n",
       " 'discussion',\n",
       " 'well',\n",
       " 'editing',\n",
       " 'wikipedia.',\n",
       " 'point',\n",
       " 'deleted',\n",
       " 'back',\n",
       " 'might',\n",
       " 'work',\n",
       " 'something',\n",
       " 'image',\n",
       " 'another',\n",
       " 'added',\n",
       " 'never',\n",
       " 'put',\n",
       " 'link',\n",
       " 'seems',\n",
       " 'stop',\n",
       " ',',\n",
       " 'blocked',\n",
       " 'feel',\n",
       " '.',\n",
       " 'list',\n",
       " 'block',\n",
       " 'right',\n",
       " 'said',\n",
       " '(utc)',\n",
       " 'using',\n",
       " 'ask',\n",
       " 'personal',\n",
       " 'fact',\n",
       " 'sure',\n",
       " 'article,',\n",
       " 'believe',\n",
       " 'hope',\n",
       " 'page,',\n",
       " 'note',\n",
       " 'actually',\n",
       " 'editors',\n",
       " 'keep',\n",
       " 'place',\n",
       " '•',\n",
       " 'remove',\n",
       " 'better',\n",
       " 'done',\n",
       " 'part',\n",
       " 'free',\n",
       " 'trying',\n",
       " 'reason',\n",
       " 'comment',\n",
       " 'it,',\n",
       " 'little',\n",
       " 'must',\n",
       " 'links',\n",
       " 'content',\n",
       " 'best',\n",
       " 'history',\n",
       " 'speedy',\n",
       " 'already',\n",
       " 'anything',\n",
       " 'give',\n",
       " 'nothing',\n",
       " 'copyright',\n",
       " 'us',\n",
       " 'let',\n",
       " 'removed',\n",
       " 'things',\n",
       " 'hi',\n",
       " 'however,',\n",
       " 'comments',\n",
       " 'last',\n",
       " 'wiki',\n",
       " 'rather',\n",
       " 'making',\n",
       " 'change',\n",
       " 'me.',\n",
       " 'come',\n",
       " 'welcome',\n",
       " 'person',\n",
       " 'anyone',\n",
       " 'question',\n",
       " 'try',\n",
       " 'understand',\n",
       " 'different',\n",
       " 'here.',\n",
       " 'got',\n",
       " 'leave',\n",
       " 'continue',\n",
       " 'every',\n",
       " '|',\n",
       " 'simply',\n",
       " 'long',\n",
       " 'found',\n",
       " 'probably',\n",
       " 'reliable',\n",
       " '(talk)',\n",
       " 'adding',\n",
       " 'you,',\n",
       " 'original',\n",
       " 'agree',\n",
       " ')',\n",
       " 'check',\n",
       " 'case',\n",
       " 'delete',\n",
       " 'tag',\n",
       " 'great',\n",
       " 'fair',\n",
       " 'subject',\n",
       " 'thing',\n",
       " 'mean',\n",
       " 'u',\n",
       " 'seem',\n",
       " 'reference',\n",
       " 'least',\n",
       " 'says',\n",
       " 'needs',\n",
       " 'problem',\n",
       " 'ip',\n",
       " 'write',\n",
       " 'show',\n",
       " 'given',\n",
       " 'editor',\n",
       " 'called',\n",
       " 'clearly',\n",
       " 'word',\n",
       " 'thought',\n",
       " 'quite',\n",
       " 'policy',\n",
       " 'tell',\n",
       " 'lot',\n",
       " 'text',\n",
       " 'far',\n",
       " 'always',\n",
       " 'also,',\n",
       " '—',\n",
       " 'whether']"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_model.stages[2].vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"\n",
      "article\n",
      "page\n",
      "please\n",
      "like\n",
      "one\n",
      "-\n",
      "wikipedia\n",
      "talk\n",
      "think\n",
      "see\n",
      "also\n",
      "know\n",
      "may\n",
      "edit\n",
      "people\n",
      "use\n",
      "get\n",
      "even\n"
     ]
    }
   ],
   "source": [
    "for i in np.argsort(-kmeans_model.clusterCenters()[1])[:20]:\n",
    "    print(preprocessing_model.stages[2].vocabulary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-\n",
      "know\n",
      "like\n",
      "page\n",
      "fuck\n",
      "article\n",
      "•\n",
      "one\n",
      "|\n",
      "please\n",
      "see\n",
      "wikipedia\n",
      "talk\n",
      "think\n",
      "\"\n",
      "must\n",
      "also\n",
      "new\n",
      "keep\n",
      "user\n",
      "first\n",
      "people\n",
      "need\n",
      "make\n",
      "time\n",
      "thanks\n",
      "good\n",
      "get\n",
      "may\n",
      "edit\n",
      "use\n",
      "it.\n",
      "added\n",
      "even\n",
      "page.\n",
      "link\n",
      "help\n",
      "many\n",
      "information\n"
     ]
    }
   ],
   "source": [
    "for i in np.argsort(-kmeans_model.clusterCenters()[0])[:40]:\n",
    "    print(preprocessing_model.stages[2].vocabulary[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The curse of dimensionality\n",
    "![curse](pics/dimensionality_vs_performance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is that?\n",
    "![curse](pics/curseofdimensionality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LDA(featuresCol=\"word_vector\", seed=5757, k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model = lda.fit(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = lda_model.transform(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='26e1b63617df36b1', comment_text='\"\\n\\n charlie wilson \\n\\ni didnt notice the music genres that were reverted. However my intention was to revert his alias that you deleted.  His alias a.k.a is actually \"\"Uncle Charlie\"\" and needs to be put back and shouldn\\'t  have been removed.\"', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, words=['\"', '', '', 'charlie', 'wilson', '', '', 'i', 'didnt', 'notice', 'the', 'music', 'genres', 'that', 'were', 'reverted.', 'however', 'my', 'intention', 'was', 'to', 'revert', 'his', 'alias', 'that', 'you', 'deleted.', '', 'his', 'alias', 'a.k.a', 'is', 'actually', '\"\"uncle', 'charlie\"\"', 'and', 'needs', 'to', 'be', 'put', 'back', 'and', \"shouldn't\", '', 'have', 'been', 'removed.\"'], words_filtered=['\"', '', '', 'charlie', 'wilson', '', '', 'didnt', 'notice', 'music', 'genres', 'reverted.', 'however', 'intention', 'revert', 'alias', 'deleted.', '', 'alias', 'a.k.a', 'actually', '\"\"uncle', 'charlie\"\"', 'needs', 'put', 'back', '', 'removed.\"'], word_vector=SparseVector(200, {0: 6.0, 1: 1.0, 70: 1.0, 78: 1.0, 101: 1.0, 179: 1.0}), topicDistribution=DenseVector([0.0129, 0.0125, 0.0121, 0.0139, 0.9363, 0.0124])),\n",
       " Row(id='11c54b3a528e642a', comment_text=\"I haven't even edited it \\n\\nGreetings. The said cabal (of which Drimes seems to be a part of, for he stalks every post I make and is the one that closed the mediation, the administration notice, the post in the discussion page) has made the Battle of Ilovaisk a semi-protected article so I have not even been able to edit it, they have instead deleted my and other peoples comments on the talk-page. As I have exhausted all options I will from now on only insult or ignore Iryna Harpy. Good game, let the propaganda flow free. I am probably done with editing for a while again. 78.68.210.173\", toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, words=['i', \"haven't\", 'even', 'edited', 'it', '', '', 'greetings.', 'the', 'said', 'cabal', '(of', 'which', 'drimes', 'seems', 'to', 'be', 'a', 'part', 'of,', 'for', 'he', 'stalks', 'every', 'post', 'i', 'make', 'and', 'is', 'the', 'one', 'that', 'closed', 'the', 'mediation,', 'the', 'administration', 'notice,', 'the', 'post', 'in', 'the', 'discussion', 'page)', 'has', 'made', 'the', 'battle', 'of', 'ilovaisk', 'a', 'semi-protected', 'article', 'so', 'i', 'have', 'not', 'even', 'been', 'able', 'to', 'edit', 'it,', 'they', 'have', 'instead', 'deleted', 'my', 'and', 'other', 'peoples', 'comments', 'on', 'the', 'talk-page.', 'as', 'i', 'have', 'exhausted', 'all', 'options', 'i', 'will', 'from', 'now', 'on', 'only', 'insult', 'or', 'ignore', 'iryna', 'harpy.', 'good', 'game,', 'let', 'the', 'propaganda', 'flow', 'free.', 'i', 'am', 'probably', 'done', 'with', 'editing', 'for', 'a', 'while', 'again.', '78.68.210.173'], words_filtered=['even', 'edited', '', '', 'greetings.', 'said', 'cabal', '(of', 'drimes', 'seems', 'part', 'of,', 'stalks', 'every', 'post', 'make', 'one', 'closed', 'mediation,', 'administration', 'notice,', 'post', 'discussion', 'page)', 'made', 'battle', 'ilovaisk', 'semi-protected', 'article', 'even', 'able', 'edit', 'it,', 'instead', 'deleted', 'peoples', 'comments', 'talk-page.', 'exhausted', 'options', 'insult', 'ignore', 'iryna', 'harpy.', 'good', 'game,', 'let', 'propaganda', 'flow', 'free.', 'probably', 'done', 'editing', 'again.', '78.68.210.173'], word_vector=SparseVector(200, {0: 2.0, 2: 1.0, 6: 1.0, 15: 1.0, 19: 2.0, 20: 1.0, 22: 1.0, 33: 1.0, 64: 1.0, 66: 1.0, 69: 1.0, 80: 1.0, 89: 1.0, 108: 1.0, 109: 1.0, 114: 1.0, 128: 1.0, 133: 1.0, 152: 1.0, 157: 1.0}), topicDistribution=DenseVector([0.4373, 0.0065, 0.079, 0.4626, 0.008, 0.0065])),\n",
       " Row(id='dba3274cda45bacb', comment_text='\"\\n\\n Deleted text \\nI note the previous deletion of the following text from the Shootdown section:\\n\"\"Earlier in the day, the Vincennes — along with Iranian gunboats — had similarly violated Omani waters until challenged by an Omani warship.\"\"\\nThe BBC documentary The Other Lockerbie (17 April 2000) covers this and includes actual video footage shot on the bridge of the Vincennes as it was challenged by the Omani vessel. I will therefore reinstate the text when the current block is lifted.   \"', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, words=['\"', '', '', 'deleted', 'text', '', 'i', 'note', 'the', 'previous', 'deletion', 'of', 'the', 'following', 'text', 'from', 'the', 'shootdown', 'section:', '\"\"earlier', 'in', 'the', 'day,', 'the', 'vincennes', '—', 'along', 'with', 'iranian', 'gunboats', '—', 'had', 'similarly', 'violated', 'omani', 'waters', 'until', 'challenged', 'by', 'an', 'omani', 'warship.\"\"', 'the', 'bbc', 'documentary', 'the', 'other', 'lockerbie', '(17', 'april', '2000)', 'covers', 'this', 'and', 'includes', 'actual', 'video', 'footage', 'shot', 'on', 'the', 'bridge', 'of', 'the', 'vincennes', 'as', 'it', 'was', 'challenged', 'by', 'the', 'omani', 'vessel.', 'i', 'will', 'therefore', 'reinstate', 'the', 'text', 'when', 'the', 'current', 'block', 'is', 'lifted.', '', '', '\"'], words_filtered=['\"', '', '', 'deleted', 'text', '', 'note', 'previous', 'deletion', 'following', 'text', 'shootdown', 'section:', '\"\"earlier', 'day,', 'vincennes', '—', 'along', 'iranian', 'gunboats', '—', 'similarly', 'violated', 'omani', 'waters', 'challenged', 'omani', 'warship.\"\"', 'bbc', 'documentary', 'lockerbie', '(17', 'april', '2000)', 'covers', 'includes', 'actual', 'video', 'footage', 'shot', 'bridge', 'vincennes', 'challenged', 'omani', 'vessel.', 'therefore', 'reinstate', 'text', 'current', 'block', 'lifted.', '', '', '\"'], word_vector=SparseVector(200, {0: 5.0, 1: 2.0, 59: 1.0, 69: 1.0, 87: 1.0, 100: 1.0, 194: 3.0, 198: 2.0}), topicDistribution=DenseVector([0.009, 0.0088, 0.356, 0.0098, 0.6076, 0.0088])),\n",
       " Row(id='5a500de632d70d5a', comment_text='Please stop.  If you continue to vandalize pages, you will be blocked from editing Wikipedia.   05:45, Jun 21, 2005 (UTC)', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, words=['please', 'stop.', '', 'if', 'you', 'continue', 'to', 'vandalize', 'pages,', 'you', 'will', 'be', 'blocked', 'from', 'editing', 'wikipedia.', '', '', '05:45,', 'jun', '21,', '2005', '(utc)'], words_filtered=['please', 'stop.', '', 'continue', 'vandalize', 'pages,', 'blocked', 'editing', 'wikipedia.', '', '', '05:45,', 'jun', '21,', '2005', '(utc)'], word_vector=SparseVector(200, {0: 3.0, 4: 1.0, 66: 1.0, 67: 1.0, 83: 1.0, 90: 1.0, 151: 1.0}), topicDistribution=DenseVector([0.0154, 0.015, 0.0145, 0.0165, 0.0184, 0.9202])),\n",
       " Row(id='fbe92b17128e0dee', comment_text='\"\\n\\nSpeedy deletion of 3spectra\\n A tag has been placed on 3spectra requesting that it be speedily deleted from Wikipedia. This has been done under section A7 of the criteria for speedy deletion, because the article appears to be about a company or corporation, but it does not indicate how or why the subject is notable: that is, why an article about that subject should be included in an encyclopedia. Under the criteria for speedy deletion, articles that do not assert the subject\\'s importance or significance may be deleted at any time. Please see the guidelines for what is generally accepted as notable, as well as our subject-specific notability guideline for companies and corporations. \\n\\nIf you think that this notice was placed here in error, you may contest the deletion by adding  to the top of the article (just below the existing speedy deletion or \"\"db\"\" tag), coupled with adding a note on the article\\'s talk page explaining your position, but be aware that once tagged for speedy deletion, if the article meets the criterion it may be deleted without delay. Please do not remove the speedy deletion tag yourself, but don\\'t hesitate to add information to the article that would would render it more in conformance with Wikipedia\\'s policies and guidelines.    \"', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, words=['\"', '', 'speedy', 'deletion', 'of', '3spectra', '', 'a', 'tag', 'has', 'been', 'placed', 'on', '3spectra', 'requesting', 'that', 'it', 'be', 'speedily', 'deleted', 'from', 'wikipedia.', 'this', 'has', 'been', 'done', 'under', 'section', 'a7', 'of', 'the', 'criteria', 'for', 'speedy', 'deletion,', 'because', 'the', 'article', 'appears', 'to', 'be', 'about', 'a', 'company', 'or', 'corporation,', 'but', 'it', 'does', 'not', 'indicate', 'how', 'or', 'why', 'the', 'subject', 'is', 'notable:', 'that', 'is,', 'why', 'an', 'article', 'about', 'that', 'subject', 'should', 'be', 'included', 'in', 'an', 'encyclopedia.', 'under', 'the', 'criteria', 'for', 'speedy', 'deletion,', 'articles', 'that', 'do', 'not', 'assert', 'the', \"subject's\", 'importance', 'or', 'significance', 'may', 'be', 'deleted', 'at', 'any', 'time.', 'please', 'see', 'the', 'guidelines', 'for', 'what', 'is', 'generally', 'accepted', 'as', 'notable,', 'as', 'well', 'as', 'our', 'subject-specific', 'notability', 'guideline', 'for', 'companies', 'and', 'corporations.', '', '', 'if', 'you', 'think', 'that', 'this', 'notice', 'was', 'placed', 'here', 'in', 'error,', 'you', 'may', 'contest', 'the', 'deletion', 'by', 'adding', '', 'to', 'the', 'top', 'of', 'the', 'article', '(just', 'below', 'the', 'existing', 'speedy', 'deletion', 'or', '\"\"db\"\"', 'tag),', 'coupled', 'with', 'adding', 'a', 'note', 'on', 'the', \"article's\", 'talk', 'page', 'explaining', 'your', 'position,', 'but', 'be', 'aware', 'that', 'once', 'tagged', 'for', 'speedy', 'deletion,', 'if', 'the', 'article', 'meets', 'the', 'criterion', 'it', 'may', 'be', 'deleted', 'without', 'delay.', 'please', 'do', 'not', 'remove', 'the', 'speedy', 'deletion', 'tag', 'yourself,', 'but', \"don't\", 'hesitate', 'to', 'add', 'information', 'to', 'the', 'article', 'that', 'would', 'would', 'render', 'it', 'more', 'in', 'conformance', 'with', \"wikipedia's\", 'policies', 'and', 'guidelines.', '', '', '', '\"'], words_filtered=['\"', '', 'speedy', 'deletion', '3spectra', '', 'tag', 'placed', '3spectra', 'requesting', 'speedily', 'deleted', 'wikipedia.', 'done', 'section', 'a7', 'criteria', 'speedy', 'deletion,', 'article', 'appears', 'company', 'corporation,', 'indicate', 'subject', 'notable:', 'is,', 'article', 'subject', 'included', 'encyclopedia.', 'criteria', 'speedy', 'deletion,', 'articles', 'assert', \"subject's\", 'importance', 'significance', 'may', 'deleted', 'time.', 'please', 'see', 'guidelines', 'generally', 'accepted', 'notable,', 'well', 'subject-specific', 'notability', 'guideline', 'companies', 'corporations.', '', '', 'think', 'notice', 'placed', 'error,', 'may', 'contest', 'deletion', 'adding', '', 'top', 'article', '(just', 'existing', 'speedy', 'deletion', '\"\"db\"\"', 'tag),', 'coupled', 'adding', 'note', \"article's\", 'talk', 'page', 'explaining', 'position,', 'aware', 'tagged', 'speedy', 'deletion,', 'article', 'meets', 'criterion', 'may', 'deleted', 'without', 'delay.', 'please', 'remove', 'speedy', 'deletion', 'tag', 'yourself,', 'hesitate', 'add', 'information', 'article', 'render', 'conformance', \"wikipedia's\", 'policies', 'guidelines.', '', '', '', '\"'], word_vector=SparseVector(200, {0: 8.0, 1: 2.0, 2: 5.0, 3: 1.0, 4: 2.0, 9: 1.0, 10: 1.0, 11: 1.0, 14: 3.0, 21: 1.0, 31: 1.0, 46: 1.0, 55: 1.0, 59: 4.0, 63: 1.0, 65: 1.0, 67: 1.0, 69: 3.0, 100: 1.0, 106: 1.0, 108: 1.0, 121: 6.0, 160: 2.0, 168: 2.0, 171: 2.0}), topicDistribution=DenseVector([0.0028, 0.0028, 0.9852, 0.003, 0.0034, 0.0028]))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.vocabSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(topic=0, termIndices=[2, 0, 3, 9, 4, 59, 14, 121, 69, 8], termWeights=[0.08926104104533482, 0.08049662990931651, 0.041972549586315355, 0.03592724599917715, 0.031607872010297106, 0.022460090874317162, 0.01975179985182996, 0.017614269436307835, 0.016819445851956244, 0.016460579858474857]),\n",
       " Row(topic=1, termIndices=[0, 1, 2, 9, 3, 105, 27, 7, 12, 90], termWeights=[0.5629140781243558, 0.06147194524571807, 0.010492120700474976, 0.009842360425537242, 0.008670961466459667, 0.008579475782924026, 0.006931165646587559, 0.006835693977168162, 0.006690663121360881, 0.006096005573499908]),\n",
       " Row(topic=2, termIndices=[0, 85, 131, 4, 45, 3, 62, 153, 9, 14], termWeights=[0.1041518502593904, 0.04009007838347178, 0.03658567901600966, 0.03560774729181142, 0.035367207683024045, 0.034888935583241465, 0.027656443714736762, 0.026402463835583072, 0.023601424238417826, 0.019444576171549457]),\n",
       " Row(topic=3, termIndices=[5, 6, 16, 0, 20, 10, 12, 22, 119, 41], termWeights=[0.05093547865439432, 0.04762137448729774, 0.024792919865717158, 0.023215249809995852, 0.020755008086165355, 0.01918469137711723, 0.017770028924907647, 0.017690843972728392, 0.016683575222642803, 0.016182084502582895]),\n",
       " Row(topic=4, termIndices=[7, 13, 0, 29, 18, 24, 11, 19, 3, 81], termWeights=[0.0510042385076669, 0.03799841965803425, 0.03415374941591341, 0.025670012676687996, 0.024665147642447798, 0.022814928815406622, 0.018696522353546827, 0.01834286511732526, 0.01777636752574786, 0.015257157100708678]),\n",
       " Row(topic=5, termIndices=[0, 1, 2, 6, 10, 7, 8, 61, 16, 5], termWeights=[0.2355927932043366, 0.08768758636710804, 0.017966064306990652, 0.017013633701135678, 0.014406794220931435, 0.011656137916945395, 0.010683883976266476, 0.010416177086016954, 0.010411079791779257, 0.010237898838738787]),\n",
       " Row(topic=6, termIndices=[0, 4, 40, 8, 17, 74, 28, 1, 15, 126], termWeights=[0.13851715499577125, 0.05291534667195064, 0.052279354278571694, 0.04500510284998403, 0.04132155866957904, 0.0325044759427235, 0.031187479247431163, 0.029890473231230907, 0.026367935501181747, 0.024214360389689715])]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.describeTopics(maxTermsPerTopic=10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "please\n",
      "fuck\n",
      "wikipedia\n",
      "use\n",
      "image\n",
      "thank\n",
      "\"\n",
      "edit\n",
      "copyright\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 4, 40, 8, 17, 74, 28, 1, 15, 126]:\n",
    "    print(preprocessing_model.stages[-1].vocabulary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "article\n",
      "\"\n",
      "page\n",
      "please\n",
      "think\n",
      "-\n",
      "may\n",
      "wikipedia\n",
      "see\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 2, 1, 3, 4, 10, 7, 14, 8, 11]:\n",
    "    print(preprocessing_model.stages[-1].vocabulary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuck\n",
      "\n",
      "fat\n",
      "shit\n",
      "suck\n",
      "go\n",
      "gay\n",
      "jew\n",
      "ass\n",
      "|\n"
     ]
    }
   ],
   "source": [
    "for i in [40, 0, 379, 257, 249, 29, 359, 567, 474, 153]:\n",
    "    print(preprocessing_model.stages[-1].vocabulary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "•\n",
      "tacos\n",
      "u\n",
      "know\n",
      "get\n",
      "name\n",
      "going\n",
      "wikipedia\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 105, 1107, 175, 13, 18, 36, 57, 8, 1]:\n",
    "    print(preprocessing_model.stages[-1].vocabulary[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering is a good dimensionality reduction technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, comment_text: string, toxic: int, severe_toxic: int, obscene: int, threat: int, insult: int, identity_hate: int, words: array<string>, words_filtered: array<string>, word_vector: vector, topicDistribution: vector]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = f.when(\n",
    "    (topics.toxic == 0) &\n",
    "    (topics.severe_toxic == 0) &\n",
    "    (topics.obscene == 0) &\n",
    "    (topics.threat == 0) &\n",
    "    (topics.insult == 0) &\n",
    "    (topics.identity_hate == 0),\n",
    "    0\n",
    ").otherwise(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_dataset = topics.withColumn(\"target\", target)[[\"id\", \"target\", \"topicDistribution\"]].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='6fdb7b6734f8bf40', target=0, topicDistribution=DenseVector([0.012, 0.0115, 0.0112, 0.3808, 0.573, 0.0116])),\n",
       " Row(id='39b742437bd11ec9', target=0, topicDistribution=DenseVector([0.5958, 0.015, 0.3392, 0.0164, 0.0185, 0.0151])),\n",
       " Row(id='9bbb8e1922fe1efb', target=0, topicDistribution=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),\n",
       " Row(id='54f9e59924682c6e', target=0, topicDistribution=DenseVector([0.6195, 0.0188, 0.0183, 0.0205, 0.3041, 0.0189])),\n",
       " Row(id='62e38775721eb79e', target=1, topicDistribution=DenseVector([0.0393, 0.0378, 0.0367, 0.3939, 0.4542, 0.038]))]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"topicDistribution\", labelCol=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = new_dataset.sampleBy(\"target\", fractions={0: 0.8, 1: 0.8}, seed=5757).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = new_dataset.join(train, on=\"id\", how=\"leftanti\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6069737271395673"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last time with CountVectorizer with 20k words in vocabulary we got 0.8275751487175559"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
