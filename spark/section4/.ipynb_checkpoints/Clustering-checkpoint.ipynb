{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.4 (default, Jan 28 2018 00:00:00)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='python3'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.4-src.zip'))\n",
    "os.environ[\"PYSPARK_PYTHON\"] = 'python3'\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![kmeans](pics/kmeans.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![kmeans_algo](pics/kmeans_algo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://master.nplcloud.com:4053\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1d6a960ac8>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/data/home/pavel.klemenkov/lectures/lecture03/toxic_comment/train.csv\")\n",
    "df.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"comment_text\", StringType()),\n",
    "    StructField(\"toxic\", IntegerType()),\n",
    "    StructField(\"severe_toxic\", IntegerType()),\n",
    "    StructField(\"obscene\", IntegerType()),\n",
    "    StructField(\"threat\", IntegerType()),\n",
    "    StructField(\"insult\", IntegerType()),\n",
    "    StructField(\"identity_hate\", IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset = spark.createDataFrame(df, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.repartition(4).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, comment_text: string, toxic: int, severe_toxic: int, obscene: int, threat: int, insult: int, identity_hate: int]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"comment_text\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stop_words = StopWordsRemover.loadDefaultStopWords(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words_filtered\", stopWords=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(inputCol=swr.getOutputCol(), outputCol=\"word_vector\", vocabSize=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preprocessing = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    swr,\n",
    "    count_vectorizer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preprocessing_model = preprocessing.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preprocessed_dataset = preprocessing_model.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(word_vector=SparseVector(200, {0: 6.0, 1: 1.0, 70: 1.0, 78: 1.0, 101: 1.0, 179: 1.0})),\n",
       " Row(word_vector=SparseVector(200, {0: 2.0, 2: 1.0, 6: 1.0, 15: 1.0, 19: 2.0, 20: 1.0, 22: 1.0, 33: 1.0, 64: 1.0, 66: 1.0, 69: 1.0, 80: 1.0, 89: 1.0, 108: 1.0, 109: 1.0, 114: 1.0, 128: 1.0, 133: 1.0, 152: 1.0, 157: 1.0})),\n",
       " Row(word_vector=SparseVector(200, {0: 5.0, 1: 2.0, 59: 1.0, 69: 1.0, 87: 1.0, 100: 1.0, 194: 3.0, 198: 2.0})),\n",
       " Row(word_vector=SparseVector(200, {0: 3.0, 4: 1.0, 66: 1.0, 67: 1.0, 83: 1.0, 90: 1.0, 151: 1.0})),\n",
       " Row(word_vector=SparseVector(200, {0: 8.0, 1: 2.0, 2: 5.0, 3: 1.0, 4: 2.0, 9: 1.0, 10: 1.0, 11: 1.0, 14: 3.0, 21: 1.0, 31: 1.0, 46: 1.0, 55: 1.0, 59: 4.0, 63: 1.0, 65: 1.0, 67: 1.0, 69: 3.0, 100: 1.0, 106: 1.0, 108: 1.0, 121: 6.0, 160: 2.0, 168: 2.0, 171: 2.0}))]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_dataset.select([\"word_vector\"]).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, comment_text: string, toxic: int, severe_toxic: int, obscene: int, threat: int, insult: int, identity_hate: int]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(featuresCol=\"word_vector\", k=6, seed=5757)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "kmeans_model = kmeans.fit(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "kmeans.fitMultiple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clustering = kmeans_model.transform(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=0),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=0),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=0),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=0),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=0),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=0),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=0),\n",
       " Row(toxic=1, severe_toxic=0, obscene=1, threat=0, insult=1, identity_hate=0, prediction=0),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=0),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=0)]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering[clustering.columns[2:8] + [\"prediction\"]].take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "evaluator = ClusteringEvaluator(featuresCol=\"word_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9531213553637657"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(comment_text='\"Contents of the library (objects and functions to be used outside, situation\\nlate August 2004)\\n\\nClasses:\\nPage: A MediaWiki page\\n    __init__               Page(Site, Title) - the page with title Title on wikimedia site Site\\n    title                  The name of the page, in a form suitable for an interwiki link\\n    urlname                The name of the page, in a form suitable for a URL\\n    titleWithoutNamespace  The name of the page, with the namespace part removed\\n    section                The section of the page (the part of the name after \\'#\\')\\n    sectionFreeTitle       The name without the section part\\n    aslink                 The name of the page in the form Title or lang:Title\\n    site                   The wiki this page is in\\n    encoding               The encoding of the page\\n    isAutoTitle            If the title is a well known, auto-translatable title\\n    autoFormat             Returns (dictName, value), where value can be a year, date, etc.,\\n                            and dictName is \\'YearBC\\', \\'December\\', etc.\\n    isCategory             True if the page is a category, false otherwise\\n    isImage                True if the page is an image, false otherwise\\n\\n    get (*)                The text of the page\\n    exists (*)             True if the page actually exists, false otherwise\\n    isRedirectPage (*)     True if the page is a redirect, false otherwise\\n    isEmpty (*)            True if the page has 4 characters or less content, not\\n                            counting interwiki and category links\\n    botMayEdit (*)         True if bot is allowed to edit page\\n    interwiki (*)          The interwiki links from the page (list of Pages)\\n    categories (*)         The categories the page is in (list of Pages)\\n    linkedPages (*)        The normal pages linked from the page (list of Pages)\\n    imagelinks (*)         The pictures on the page (list of ImagePages)\\n    templates (*)          All templates referenced on the page (list of strings)\\n    getRedirectTarget (*)  The page the page redirects to\\n    isDisambig (*)         True if the page is a disambiguation page\\n    getReferences          List of pages linking to the page\\n    namespace              The namespace in which the page is\\n    permalink (*)          The url of the permalink of the current version\\n    move                   Move the page to another title\\n    put(newtext)           Saves the page\\n    put_async(newtext)     Queues the page to be saved asynchronously\\n    delete                 Deletes the page (requires being logged in)\\n\\n    (*)  This loads the page if it has not been loaded before; permalink might\\n          even reload it if it has been loaded before\\n\\nSite: a MediaWiki site\\n    messages               There are new messages on the site\\n    forceLogin()           Does not continue until the user has logged in to\\n                            the site\\n    getUrl()               Retrieve an URL from the site\\n    mediawiki_message(key): Retrieve the text of the MediaWiki message with\\n                            the key \"\"key\"\"\\n    has_mediawiki_message(key)  True if this site defines a MediaWiki message\\n                                 with the key \"\"key\"\"\\n    Special pages:\\n        Dynamic pages:\\n            allpages(): Special:Allpages\\n            newpages(): Special:Newpages\\n            longpages(): Special:Longpages\\n            shortpages(): Special:Shortpages\\n            categories(): Special:Categories\\n\\n        Cached pages:\\n            deadendpages(): Special:Deadendpages\\n            ancientpages(): Special:Ancientpages\\n            lonelypages(): Special:Lonelypages\\n            uncategorizedcategories(): Special:Uncategorizedcategories\\n            uncategorizedpages(): Special:Uncategorizedpages\\n            unusedcategories(): Special:Unusuedcategories\\n\\nOther functions:\\ngetall(): Load pages via Special:Export\\nsetAction(text): Use \\'text\\' instead of \"\"Wikipedia python library\"\" in\\n    editsummaries\\nhandleArgs(): Checks whether text is an argument defined on wikipedia.py\\n    (these are -family, -lang, -log and others)\\ntranslate(xx, dict): dict is a dictionary, giving text depending on language,\\n    xx is a language. Returns the text in the most applicable language for\\n    the xx: wiki\\nsetUserAgent(text): Sets the string being passed to the HTTP server as\\n    the User-agent: header. Defaults to \\'Pywikipediabot/1.0\\'.\\n\\noutput(text): Prints the text \\'text\\' in the encoding of the user\\'s console.NEWL'),\n",
       " Row(comment_text='Lots More Abstracts\\nSmithsonian/NASA Astrophysics Data System (ADS)\\nQuery Results from the Instrumentation Database\\nRetrieved 100 abstracts, starting with number 1. Total number selected: 637550.\\n-\\n Bibcode \\n Score Date List of Links \\n Authors \\n Title \\n Access Control Help \\n\\n \\n1  1977OSAJ...67..399G \\n 1.000 03/1977 A                                              C                      U       \\n Grosso, R. P.; Yellin, M. \\n The membrane mirror as an adaptive optical element \\n\\n \\n2  1976SPIE...75...97Y \\n 0.456 00/1976 A                                  T  M                                       \\n Yellin, M. \\n Using membrane mirrors in adaptive optics \\n\\n \\n3  1993SPIE.1945..421M \\n 0.367 11/1993 A                                  T  M                              U       \\n Miller, Linda M.; Agronin, Michael L.; Bartman, Randall K.; Kaiser, William J.; Kenny, Thomas W.; Norton, Robert L.; Vote, Erika C. \\n Fabrication and characterization of a micromachined deformable mirror for adaptive optics applications \\n\\n \\n4  1991SPIE.1542..165C \\n 0.331 12/1991 A                                  T  M      C                               \\n Clampin, M.; Durrance, S. T.; Golimowski, D. A.; Barkhouser, R. H. \\n The Johns Hopkins Adaptive Optics Coronagraph \\n\\n \\n5  2000SPIE.4091...83Y \\n 0.322 10/2000 A                                  T  M                              U       \\n Yang, Eui-Hyeok; Wiberg, Dean V.; Dekany, Richard G. \\n Design and fabrication of electrostatic actuators with corrugated membranes for MEMS deformable mirror in space \\n\\n \\n6  2000SPIE.4075...41R \\n 0.321 09/2000 A                                  T  M                              U       \\n Ross, Alan W.; Graham, Stephen C.; Gundlach, Alan M.; Stevenson, J. Tom M.; Hossack, William J.; Vass, David G.; Bodammer, Georg; Smith, Euan; Ward, Kevin \\n Microfabrication and packaging of deformable mirror devices \\n\\n \\n7  1994SPIE.2201..762T \\n 0.315 05/1994 A                                  T  M      C                      U       \\n Takami, Hideki; Iye, Masanori \\n Membrane deformable mirror for SUBARU adaptive optics \\n\\n \\n8  1999SPIE.3785..160W \\n 0.315 10/1999 A                                  T  M                                       \\n Winsor, Robert S.; Sivaramakrishnan, Anand; Makidon, Russell B. \\n Finite element analysis of low-cost membrane deformable mirrors for high-order adaptive optics \\n\\n \\n9  2000SPIE.4348..348B \\n 0.302 02/2000 A                                  T  M                                       \\n Borovkov, Alexei I.; Pyatishev, Evgenij N.; Lurie, Mihail S.; Korshunov, Andrey V.; Akulshin, Y. D.; Dolganov, A. G.; Sabadash, V. O. \\n Micronozzles: 3D numerical structural and gas dynamics modeling, fabrication, and preliminary experimental results \\n\\n \\n10  1999SPIE.3591..137Z \\n 0.298 06/1999 A                                  T  M                                       \\n Zhu, Lijun; Sun, Pang Chen; Bartsch, Dirk-Uwe G.; Freeman, William R.; Fainman, Yeshaiahu \\n Adaptive fundus imaging using a micromachined membrane deformable mirror \\n\\n \\n11  2001SPIE.4327...13W \\n 0.297 08/2001 A                                  T  M      C                               \\n Wagner, John W.; Agnes, Gregory S. \\n Optical metrology of adaptive membrane mirrors \\n\\n \\n12  2002OptEn..41..561P \\n 0.297 03/2002 A      E  F                              R                          U       \\n Perreault, Julie A.; Bifano, Thomas G.; Levine, Bruce M.; Horenstein, Mark N. \\n Adaptive optic'),\n",
       " Row(comment_text='I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg'),\n",
       " Row(comment_text='ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \\nAN'),\n",
       " Row(comment_text='....... \\nnow that i think about it, luna means moon and satin means devil!\\nso that means you are the moon devil who else would be the moon devil\\nbut the person who disabled my ACCOUNT!!! ( if trevor is reading this \\nthen i want to say that i love that snowman, i tred to make one but it\\ndidnt work(scroll down))\\n\\nfurther\\n\\neven further\\n\\nalmost there....\\n\\n __\\n(**)\\n( .)\\n(. )\\n                          O   \\n                         O O                  \\n                        O   O                        \\n                       O     O                            \\n                      O       O                      \\n                     O         O                       \\n                    O           O                     \\n                   O             O               \\n                  O               O                                                             \\n                 O                 O          \\n                O                   O       \\n               O                     O         \\n              O                       O         \\n             O                         O\\n            OOOOOOOOOOOOOOOOOOOOOOOOOOOO                               \\n           O  O                      O  O                             \\n          O    O                    O    O                    \\n         O      O                  O      O                        \\n        O        O                O        O                         \\n       O          O              O          O               \\n      O            O            O            O              \\n     O              O          O              O     \\n    O                O        O                O                     \\n   O                  O      O                  O                                    \\n  O                    O    O                    O                                               \\n O                      O  O                      O                          \\nO                        OO                        O \\nOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO  SKILLS! IT TOOK 20 MINUTES TOO MAKE')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering.filter(clustering.prediction == 1)[[\"comment_text\"]].take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(featuresCol=\"word_vector\", k=2, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "kmeans_model = kmeans.fit(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clustering = kmeans_model.transform(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9994799602125465"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([3.06700217, 0.51181359, 0.2445131 , 0.17984859, 0.17241574,\n",
       "        0.16505183, 0.15437886, 0.14628796, 0.14061619, 0.14033416,\n",
       "        0.11887542, 0.11762826, 0.10257455, 0.10253694, 0.09565561,\n",
       "        0.09482834, 0.09417029, 0.09082363, 0.08224389, 0.08131009,\n",
       "        0.07925446, 0.07138291, 0.07052431, 0.06881338, 0.06504055,\n",
       "        0.0631416 , 0.06251488, 0.06223286, 0.06039032, 0.05929357,\n",
       "        0.05810281, 0.05765784, 0.05692458, 0.05540793, 0.05475614,\n",
       "        0.05465587, 0.05432371, 0.05344004, 0.05312042, 0.05281333,\n",
       "        0.05280706, 0.05275692, 0.0525125 , 0.05196726, 0.05142202,\n",
       "        0.05116506, 0.05008711, 0.04930372, 0.04917838, 0.04870207,\n",
       "        0.04813176, 0.04769306, 0.04764919, 0.04754892, 0.04740477,\n",
       "        0.0460824 , 0.04578784, 0.04533661, 0.04529274, 0.04521753,\n",
       "        0.04462215, 0.04422106, 0.04403931, 0.04393903, 0.04333739,\n",
       "        0.04328725, 0.04289242, 0.04227824, 0.04098094, 0.04091826,\n",
       "        0.04075532, 0.04057984, 0.04027901, 0.0400722 , 0.03988418,\n",
       "        0.03876236, 0.03848661, 0.03846781, 0.0379727 , 0.03737732,\n",
       "        0.03730211, 0.03718931, 0.03703263, 0.03687595, 0.03676941,\n",
       "        0.03661273, 0.03648112, 0.03649365, 0.03648112, 0.03639964,\n",
       "        0.0362179 , 0.03616149, 0.03586694, 0.03554104, 0.03549717,\n",
       "        0.03549091, 0.03507101, 0.03482659, 0.03406199, 0.03391785,\n",
       "        0.03329113, 0.0328399 , 0.03222572, 0.03186849, 0.03177448,\n",
       "        0.03145486, 0.03126058, 0.03122297, 0.03099109, 0.03094722,\n",
       "        0.03080307, 0.03073413, 0.03058999, 0.03054612, 0.03037064,\n",
       "        0.03033304, 0.03023903, 0.03018262, 0.03013875, 0.0298442 ,\n",
       "        0.02965618, 0.02954964, 0.0294995 , 0.02941803, 0.02928642,\n",
       "        0.02927389, 0.02915481, 0.02911094, 0.02894173, 0.02891039,\n",
       "        0.0285845 , 0.02845289, 0.0281646 , 0.028127  , 0.02808939,\n",
       "        0.02802672, 0.02787631, 0.02787631, 0.02780111, 0.0277635 ,\n",
       "        0.0276695 , 0.02745641, 0.02728093, 0.02719319, 0.02696131,\n",
       "        0.02666675, 0.02657901, 0.02651007, 0.02640353, 0.02620298,\n",
       "        0.02618418, 0.02613404, 0.02603377, 0.02595229, 0.02570161,\n",
       "        0.02554493, 0.02545092, 0.02542585, 0.02539452, 0.02526291,\n",
       "        0.02512503, 0.02494328, 0.02469886, 0.02458605, 0.02431657,\n",
       "        0.02409095, 0.02369612, 0.02365225, 0.02355824, 0.02353944,\n",
       "        0.02323862, 0.02323235, 0.02293153, 0.02291272, 0.02266204,\n",
       "        0.02269964, 0.02261817, 0.02248656, 0.02244269, 0.02243015,\n",
       "        0.02232361, 0.02228601, 0.02226094, 0.02220453, 0.02202279,\n",
       "        0.02201025, 0.02197265, 0.02184104, 0.02170943, 0.02165303,\n",
       "        0.02157155, 0.02152141, 0.02146501, 0.02140234, 0.0212958 ,\n",
       "        0.02130206, 0.02119552, 0.02113285, 0.02109525, 0.02105138]),\n",
       " array([1.06677778e+03, 4.44444444e-01, 1.11111111e-01, 3.33333333e+00,\n",
       "        1.11111111e-01, 1.11111111e-01, 1.11111111e-01, 3.11111111e+00,\n",
       "        1.11111111e-01, 0.00000000e+00, 2.22222222e-01, 1.11111111e-01,\n",
       "        3.33333333e-01, 4.44444444e-01, 0.00000000e+00, 1.11111111e-01,\n",
       "        0.00000000e+00, 2.22222222e-01, 2.22222222e-01, 2.22222222e-01,\n",
       "        2.22222222e-01, 0.00000000e+00, 1.11111111e-01, 1.11111111e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.11111111e-01, 1.11111111e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.11111111e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.11111111e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.66666667e-01, 0.00000000e+00, 0.00000000e+00, 1.11111111e-01,\n",
       "        0.00000000e+00, 1.11111111e-01, 1.11111111e-01, 1.11111111e-01,\n",
       "        0.00000000e+00, 1.11111111e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.22222222e-01, 1.11111111e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.22222222e-01, 3.33333333e-01,\n",
       "        4.44444444e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.11111111e-01,\n",
       "        0.00000000e+00, 1.11111111e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.11111111e-01,\n",
       "        1.11111111e-01, 0.00000000e+00, 0.00000000e+00, 1.11111111e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.11111111e-01,\n",
       "        1.11111111e-01, 0.00000000e+00, 1.11111111e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.11111111e-01, 3.33333333e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.33333333e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.33333333e-01,\n",
       "        0.00000000e+00, 1.11111111e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.33333333e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.11111111e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.33333333e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.11111111e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.11111111e-01, 2.22222222e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.11111111e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.11111111e-01,\n",
       "        1.11111111e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.11111111e-01, 1.11111111e-01, 2.22222222e-01, 1.11111111e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.66666667e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.11111111e-01, 1.11111111e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.11111111e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.66666667e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.11111111e-01])]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_model.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0109564999373197"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(kmeans_model.clusterCenters()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  41,  42,  43,  44,  45,  40,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  87,  86,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 125, 124, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 137, 136, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155, 156,\n",
       "       157, 158, 159, 153, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 175, 174, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 195,\n",
       "       194, 196, 197, 198, 199])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(-kmeans_model.clusterCenters()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   3,   7, 194, 174,  36,   1,  56,  13, 117, 109,  91,  86,\n",
       "        55,  99,  12, 135, 166,  49,  20,  54,  18,  17,  10,  19,  63,\n",
       "        65,  71,   6,  75,  79,  80,  82,  85,   8, 101,  72,  15,   5,\n",
       "        11, 134, 142, 151, 152, 164, 165, 167,   4, 180, 181, 186,   2,\n",
       "       114, 129, 199,  26,  27,  39,  41,  42,  43,  30,  23,  22,  45,\n",
       "        50,  33, 140, 132, 160, 159, 158, 133, 157, 156, 155, 154, 153,\n",
       "        32,  29, 136, 137, 150, 149, 148, 161, 138, 147, 146, 145, 144,\n",
       "       143,  31, 141, 139, 162,  52,  28, 197, 196, 195,  16, 193, 192,\n",
       "       191, 190, 189, 188, 187,  21, 185, 184, 163, 183, 179, 178, 177,\n",
       "       176, 175,  24, 173, 172, 171, 170, 169, 168,  25, 131, 182, 130,\n",
       "       127, 128,  40,  90,  89,  88,  87,   9,  84,  83,  81,  44,  78,\n",
       "        77,  76,  74,  73,  46,  47,  51,  57,  58,  59,  60,  61,  92,\n",
       "        62,  48,  66,  67,  68,  69,  70,  64,  34,  93,  95,  53, 126,\n",
       "       125, 124, 123, 122, 121, 120, 119, 118,  35, 116, 115,  37, 113,\n",
       "       112, 111,  96,  97,  98, 198, 100, 102,  94, 103, 105, 106,  14,\n",
       "       108,  38, 110, 104, 107])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(-kmeans_model.clusterCenters()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '\"',\n",
       " 'article',\n",
       " 'page',\n",
       " 'please',\n",
       " 'like',\n",
       " 'one',\n",
       " '-',\n",
       " 'wikipedia',\n",
       " 'talk',\n",
       " 'think',\n",
       " 'see',\n",
       " 'also',\n",
       " 'know',\n",
       " 'may',\n",
       " 'edit',\n",
       " 'people',\n",
       " 'use',\n",
       " 'get',\n",
       " 'even',\n",
       " 'make',\n",
       " 'articles',\n",
       " 'good',\n",
       " 'want',\n",
       " 'time',\n",
       " 'it.',\n",
       " 'need',\n",
       " 'new',\n",
       " 'thank',\n",
       " 'go',\n",
       " 'first',\n",
       " 'information',\n",
       " 'many',\n",
       " 'made',\n",
       " 'find',\n",
       " 'page.',\n",
       " 'name',\n",
       " 'really',\n",
       " 'thanks',\n",
       " 'say',\n",
       " 'fuck',\n",
       " 'much',\n",
       " 'used',\n",
       " 'since',\n",
       " 'article.',\n",
       " 'user',\n",
       " 'add',\n",
       " 'way',\n",
       " 'take',\n",
       " 'help',\n",
       " 'sources',\n",
       " 'look',\n",
       " 'someone',\n",
       " 'still',\n",
       " 'read',\n",
       " 'section',\n",
       " 'pages',\n",
       " 'going',\n",
       " 'two',\n",
       " 'deletion',\n",
       " 'you.',\n",
       " 'source',\n",
       " 'edits',\n",
       " 'without',\n",
       " 'discussion',\n",
       " 'well',\n",
       " 'editing',\n",
       " 'wikipedia.',\n",
       " 'point',\n",
       " 'deleted',\n",
       " 'back',\n",
       " 'might',\n",
       " 'work',\n",
       " 'something',\n",
       " 'image',\n",
       " 'another',\n",
       " 'added',\n",
       " 'never',\n",
       " 'put',\n",
       " 'link',\n",
       " 'seems',\n",
       " 'stop',\n",
       " ',',\n",
       " 'blocked',\n",
       " 'feel',\n",
       " '.',\n",
       " 'list',\n",
       " 'block',\n",
       " 'right',\n",
       " 'said',\n",
       " '(utc)',\n",
       " 'using',\n",
       " 'ask',\n",
       " 'personal',\n",
       " 'fact',\n",
       " 'sure',\n",
       " 'article,',\n",
       " 'believe',\n",
       " 'hope',\n",
       " 'page,',\n",
       " 'note',\n",
       " 'actually',\n",
       " 'editors',\n",
       " 'keep',\n",
       " 'place',\n",
       " 'â€¢',\n",
       " 'remove',\n",
       " 'better',\n",
       " 'done',\n",
       " 'part',\n",
       " 'free',\n",
       " 'trying',\n",
       " 'reason',\n",
       " 'comment',\n",
       " 'it,',\n",
       " 'little',\n",
       " 'must',\n",
       " 'links',\n",
       " 'content',\n",
       " 'best',\n",
       " 'history',\n",
       " 'speedy',\n",
       " 'already',\n",
       " 'anything',\n",
       " 'give',\n",
       " 'nothing',\n",
       " 'copyright',\n",
       " 'us',\n",
       " 'let',\n",
       " 'removed',\n",
       " 'things',\n",
       " 'hi',\n",
       " 'however,',\n",
       " 'comments',\n",
       " 'last',\n",
       " 'wiki',\n",
       " 'rather',\n",
       " 'making',\n",
       " 'change',\n",
       " 'me.',\n",
       " 'come',\n",
       " 'welcome',\n",
       " 'person',\n",
       " 'anyone',\n",
       " 'question',\n",
       " 'try',\n",
       " 'understand',\n",
       " 'different',\n",
       " 'here.',\n",
       " 'got',\n",
       " 'leave',\n",
       " 'continue',\n",
       " 'every',\n",
       " '|',\n",
       " 'simply',\n",
       " 'long',\n",
       " 'found',\n",
       " 'probably',\n",
       " 'reliable',\n",
       " '(talk)',\n",
       " 'adding',\n",
       " 'you,',\n",
       " 'original',\n",
       " 'agree',\n",
       " ')',\n",
       " 'check',\n",
       " 'case',\n",
       " 'delete',\n",
       " 'tag',\n",
       " 'great',\n",
       " 'fair',\n",
       " 'subject',\n",
       " 'thing',\n",
       " 'mean',\n",
       " 'u',\n",
       " 'seem',\n",
       " 'reference',\n",
       " 'least',\n",
       " 'says',\n",
       " 'needs',\n",
       " 'problem',\n",
       " 'ip',\n",
       " 'write',\n",
       " 'show',\n",
       " 'given',\n",
       " 'editor',\n",
       " 'called',\n",
       " 'clearly',\n",
       " 'word',\n",
       " 'thought',\n",
       " 'quite',\n",
       " 'policy',\n",
       " 'tell',\n",
       " 'lot',\n",
       " 'text',\n",
       " 'far',\n",
       " 'always',\n",
       " 'also,',\n",
       " 'â€”',\n",
       " 'whether']"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_model.stages[2].vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|\n",
      "page\n",
      "-\n",
      "\"\n",
      "get\n",
      "know\n",
      "(talk)\n",
      "article\n",
      "name\n",
      "section\n",
      "way\n",
      "one\n",
      "make\n",
      "u\n",
      "edit\n",
      "people\n",
      "like\n",
      "come\n",
      "take\n"
     ]
    }
   ],
   "source": [
    "for i in np.argsort(-kmeans_model.clusterCenters()[1])[:20]:\n",
    "    print(preprocessing_model.stages[2].vocabulary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"\n",
      "article\n",
      "page\n",
      "please\n",
      "like\n",
      "one\n",
      "-\n",
      "wikipedia\n",
      "talk\n",
      "think\n",
      "see\n",
      "also\n",
      "know\n",
      "may\n",
      "edit\n",
      "people\n",
      "use\n",
      "get\n",
      "even\n"
     ]
    }
   ],
   "source": [
    "for i in np.argsort(-kmeans_model.clusterCenters()[0])[:20]:\n",
    "    print(preprocessing_model.stages[2].vocabulary[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The curse of dimensionality\n",
    "![curse](pics/dimensionality_vs_performance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why is that?\n",
    "![curse](pics/curseofdimensionality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lda = LDA(featuresCol=\"word_vector\", seed=5757, k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lda_model = lda.fit(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "topics = lda_model.transform(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='6fdb7b6734f8bf40', comment_text='\"\\n\\n\"\"Katara\"\"\\nI\\'ve removed the section entirely. I don\\'t care if you like to pretend that Katara and Zuko are meant for each other. It\\'s still not case, and there has been no indication whatsoever. Thus, there\\'s little point to actually have the section and exempt Toph and Sokka beyond the insane delusions of shippers.  \"', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, words=['\"', '', '\"\"katara\"\"', \"i've\", 'removed', 'the', 'section', 'entirely.', 'i', \"don't\", 'care', 'if', 'you', 'like', 'to', 'pretend', 'that', 'katara', 'and', 'zuko', 'are', 'meant', 'for', 'each', 'other.', \"it's\", 'still', 'not', 'case,', 'and', 'there', 'has', 'been', 'no', 'indication', 'whatsoever.', 'thus,', \"there's\", 'little', 'point', 'to', 'actually', 'have', 'the', 'section', 'and', 'exempt', 'toph', 'and', 'sokka', 'beyond', 'the', 'insane', 'delusions', 'of', 'shippers.', '', '\"'], words_filtered=['\"', '', '\"\"katara\"\"', 'removed', 'section', 'entirely.', 'care', 'like', 'pretend', 'katara', 'zuko', 'meant', 'other.', 'still', 'case,', 'indication', 'whatsoever.', 'thus,', 'little', 'point', 'actually', 'section', 'exempt', 'toph', 'sokka', 'beyond', 'insane', 'delusions', 'shippers.', '', '\"'], word_vector=SparseVector(200, {0: 2.0, 1: 2.0, 5: 1.0, 53: 1.0, 55: 2.0, 68: 1.0, 101: 1.0, 115: 1.0, 129: 1.0}), topicDistribution=DenseVector([0.0119, 0.7266, 0.0114, 0.2247, 0.014, 0.0115])),\n",
       " Row(id='39b742437bd11ec9', comment_text=\"If that article says its legal the article is incorrect. Evidence of this is that Italy has issued warrants for the arrest of 13 CIA agents who kidnapped an Italian citizen. `Extradition' is the legal process whereby a person may be handed over to a foreign legal system. It involves the case for extradition being presented to a judge. What the CIA do is grab someone off the street and ship them abroad for torture. Under the law, that is kidnap. If members of european security agencies were involved they are also kidnappers, under the law. There are several cases of CIA 'extraordinary rendition' being investigated across Europe at the moment. All are treated as illegal. ... al  bin Baloney (Hows my driving?)\", toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, words=['if', 'that', 'article', 'says', 'its', 'legal', 'the', 'article', 'is', 'incorrect.', 'evidence', 'of', 'this', 'is', 'that', 'italy', 'has', 'issued', 'warrants', 'for', 'the', 'arrest', 'of', '13', 'cia', 'agents', 'who', 'kidnapped', 'an', 'italian', 'citizen.', \"`extradition'\", 'is', 'the', 'legal', 'process', 'whereby', 'a', 'person', 'may', 'be', 'handed', 'over', 'to', 'a', 'foreign', 'legal', 'system.', 'it', 'involves', 'the', 'case', 'for', 'extradition', 'being', 'presented', 'to', 'a', 'judge.', 'what', 'the', 'cia', 'do', 'is', 'grab', 'someone', 'off', 'the', 'street', 'and', 'ship', 'them', 'abroad', 'for', 'torture.', 'under', 'the', 'law,', 'that', 'is', 'kidnap.', 'if', 'members', 'of', 'european', 'security', 'agencies', 'were', 'involved', 'they', 'are', 'also', 'kidnappers,', 'under', 'the', 'law.', 'there', 'are', 'several', 'cases', 'of', 'cia', \"'extraordinary\", \"rendition'\", 'being', 'investigated', 'across', 'europe', 'at', 'the', 'moment.', 'all', 'are', 'treated', 'as', 'illegal.', '...', 'al', '', 'bin', 'baloney', '(hows', 'my', 'driving?)'], words_filtered=['article', 'says', 'legal', 'article', 'incorrect.', 'evidence', 'italy', 'issued', 'warrants', 'arrest', '13', 'cia', 'agents', 'kidnapped', 'italian', 'citizen.', \"`extradition'\", 'legal', 'process', 'whereby', 'person', 'may', 'handed', 'foreign', 'legal', 'system.', 'involves', 'case', 'extradition', 'presented', 'judge.', 'cia', 'grab', 'someone', 'street', 'ship', 'abroad', 'torture.', 'law,', 'kidnap.', 'members', 'european', 'security', 'agencies', 'involved', 'also', 'kidnappers,', 'law.', 'several', 'cases', 'cia', \"'extraordinary\", \"rendition'\", 'investigated', 'across', 'europe', 'moment.', 'treated', 'illegal.', '...', 'al', '', 'bin', 'baloney', '(hows', 'driving?)'], word_vector=SparseVector(200, {0: 1.0, 2: 2.0, 12: 1.0, 14: 1.0, 52: 1.0, 142: 1.0, 166: 1.0, 178: 1.0}), topicDistribution=DenseVector([0.6907, 0.015, 0.2444, 0.0166, 0.0184, 0.015])),\n",
       " Row(id='9bbb8e1922fe1efb', comment_text='(in the Thilo Sarrazin tradition)', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, words=['(in', 'the', 'thilo', 'sarrazin', 'tradition)'], words_filtered=['(in', 'thilo', 'sarrazin', 'tradition)'], word_vector=SparseVector(200, {}), topicDistribution=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),\n",
       " Row(id='54f9e59924682c6e', comment_text='Flamboyant (gay) article and wikilinks \\n\\ncreate appropriate page for flaming, flamer, flamboyant as adjective and wikilink potent edits from this from mid-September 2007 going back in time.Benjiboi', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, words=['flamboyant', '(gay)', 'article', 'and', 'wikilinks', '', '', 'create', 'appropriate', 'page', 'for', 'flaming,', 'flamer,', 'flamboyant', 'as', 'adjective', 'and', 'wikilink', 'potent', 'edits', 'from', 'this', 'from', 'mid-september', '2007', 'going', 'back', 'in', 'time.benjiboi'], words_filtered=['flamboyant', '(gay)', 'article', 'wikilinks', '', '', 'create', 'appropriate', 'page', 'flaming,', 'flamer,', 'flamboyant', 'adjective', 'wikilink', 'potent', 'edits', 'mid-september', '2007', 'going', 'back', 'time.benjiboi'], word_vector=SparseVector(200, {0: 2.0, 2: 1.0, 3: 1.0, 57: 1.0, 62: 1.0, 70: 1.0}), topicDistribution=DenseVector([0.6402, 0.0188, 0.0185, 0.0206, 0.2829, 0.0188])),\n",
       " Row(id='62e38775721eb79e', comment_text=\"| decline=Niggers, jews, bad news! Also my cock is hard so it's time for rape lol 86.181.0.14\", toxic=1, severe_toxic=1, obscene=1, threat=0, insult=1, identity_hate=1, words=['|', 'decline=niggers,', 'jews,', 'bad', 'news!', 'also', 'my', 'cock', 'is', 'hard', 'so', \"it's\", 'time', 'for', 'rape', 'lol', '86.181.0.14'], words_filtered=['|', 'decline=niggers,', 'jews,', 'bad', 'news!', 'also', 'cock', 'hard', 'time', 'rape', 'lol', '86.181.0.14'], word_vector=SparseVector(200, {12: 1.0, 24: 1.0, 153: 1.0}), topicDistribution=DenseVector([0.0389, 0.7977, 0.0373, 0.0415, 0.0468, 0.0379]))]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.vocabSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://master.nplcloud.com:4053\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1d6a960ac8>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'setCallSite'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-248-54f6165bb84f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribeTopics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \"\"\"\n\u001b[0;32m--> 504\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \"\"\"\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/traceback_utils.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark_stack_depth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_site\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark_stack_depth\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'setCallSite'"
     ]
    }
   ],
   "source": [
    "lda_model.describeTopics().take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nigger\n",
      "fucking\n",
      "like\n",
      "moron\n",
      "sucks\n",
      "redirect\n",
      "hi\n",
      "dick\n",
      "jews\n",
      "fucksex\n"
     ]
    }
   ],
   "source": [
    "for i in [214, 211, 5, 463, 582, 334, 131, 751, 700, 1355]:\n",
    "    print(preprocessing_model.stages[-1].vocabulary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "article\n",
      "\"\n",
      "page\n",
      "please\n",
      "think\n",
      "-\n",
      "may\n",
      "wikipedia\n",
      "see\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 2, 1, 3, 4, 10, 7, 14, 8, 11]:\n",
    "    print(preprocessing_model.stages[-1].vocabulary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuck\n",
      "\n",
      "fat\n",
      "shit\n",
      "suck\n",
      "go\n",
      "gay\n",
      "jew\n",
      "ass\n",
      "|\n"
     ]
    }
   ],
   "source": [
    "for i in [40, 0, 379, 257, 249, 29, 359, 567, 474, 153]:\n",
    "    print(preprocessing_model.stages[-1].vocabulary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â€¢\n",
      "tacos\n",
      "u\n",
      "know\n",
      "get\n",
      "name\n",
      "going\n",
      "wikipedia\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 105, 1107, 175, 13, 18, 36, 57, 8, 1]:\n",
    "    print(preprocessing_model.stages[-1].vocabulary[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Clustering is a good dimensionality reduction technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, comment_text: string, toxic: int, severe_toxic: int, obscene: int, threat: int, insult: int, identity_hate: int, words: array<string>, words_filtered: array<string>, word_vector: vector, topicDistribution: vector]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "target = f.when(\n",
    "    (topics.toxic == 0) &\n",
    "    (topics.severe_toxic == 0) &\n",
    "    (topics.obscene == 0) &\n",
    "    (topics.threat == 0) &\n",
    "    (topics.insult == 0) &\n",
    "    (topics.identity_hate == 0),\n",
    "    0\n",
    ").otherwise(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_dataset = topics.withColumn(\"target\", target)[[\"id\", \"target\", \"topicDistribution\"]].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='6fdb7b6734f8bf40', target=0, topicDistribution=DenseVector([0.0052, 0.9694, 0.0053, 0.0082, 0.0056, 0.0063])),\n",
       " Row(id='39b742437bd11ec9', target=0, topicDistribution=DenseVector([0.0025, 0.0031, 0.0025, 0.0039, 0.0027, 0.9853])),\n",
       " Row(id='9bbb8e1922fe1efb', target=0, topicDistribution=DenseVector([0.0665, 0.0836, 0.0674, 0.1029, 0.0708, 0.6088])),\n",
       " Row(id='54f9e59924682c6e', target=0, topicDistribution=DenseVector([0.009, 0.0113, 0.0091, 0.9501, 0.0096, 0.0108])),\n",
       " Row(id='62e38775721eb79e', target=1, topicDistribution=DenseVector([0.0127, 0.0159, 0.4886, 0.0196, 0.0135, 0.4497]))]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"topicDistribution\", labelCol=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = new_dataset.sampleBy(\"target\", fractions={0: 0.8, 1: 0.8}, seed=5757).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test = new_dataset.join(train, on=\"id\", how=\"leftanti\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr_model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8381544862513701"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Last time with CountVectorizer with 20k words in vocabulary we got 0.8275751487175559"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
